{
  "hash": "eca724431a70e84014ed37bd457d2514",
  "result": {
    "markdown": "---\ntitle: \"Introduction and examples\"\n---\n\n\n## Summary\n\nWe begin by considering examples within two broad themes: the replication crisis in science and fairness and inequality in algorithmic or data-driven systems.\n\n## References\n\n### Assigned reading\n\n- [Book chapter](https://joshualoftus.com/ms4ds/ethical-data-science.html)\n- [Wikipedia: Replication crisis](https://en.wikipedia.org/wiki/Replication_crisis)\n\n### Additional references\n\n#### Replication crisis\n\n- [Why Most Published Research Findings Are False](https://journals.plos.org/plosmedicine/article?id=10.1371/journal.pmed.0020124)\n- [Many Analysts, One Dataset](https://journals.sagepub.com/doi/10.1177/2515245917747646)\n- [Estimating the reproducibility of psychological science](https://www.science.org/doi/10.1126/science.aac4716)\n- [Likelihood of Null Effects of Large NHLBI Clinical Trials Has Increased over Time](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0132382)\n- [Survey on Reproducibility Crisis](https://www.nature.com/articles/533452a)\n\n#### (Un)fair algorithms\n\n- [Redlining](https://en.wikipedia.org/wiki/Redlining), [Amazonâ€™s same day delivery](https://www.bloomberg.com/graphics/2016-amazon-same-day/), and [car insurance premiums](https://www.propublica.org/article/minority-neighborhoods-higher-car-insurance-premiums-white-areas-same-risk)\n- [Guardian series on automating poverty](https://www.theguardian.com/technology/series/automating-poverty)\n- [Racial bias in personalized medicine](https://www.science.org/doi/10.1126/science.aax2342)\n\n## Computer setup\n\n###  Installing `R` and `RStudio`\n\nFirst [install R](https://cran.r-project.org/) and then install [RStudio](https://www.rstudio.com/products/rstudio/download/) (this second step is highly recommended but not required, if you prefer another IDE and you're sure you know what you're doing). Finally, open RStudio and install the [tidyverse](https://www.tidyverse.org/packages/) set of packages by running the command\n\n```\ninstall.packages(\"tidyverse\")\n```\n\n**Note**: If you use a Mac or Linux-based computer you may want to install these using a package manager instead of downloading them from the websites linked above. Personally, on a Mac computer I use [Homebrew](https://brew.sh/) (the link has instructions for how to install it) to [install R](https://formulae.brew.sh/cask/r) and [RStudio](https://formulae.brew.sh/cask/rstudio).\n\n### Resources for learning `R`\n\n- [RStudio blog post](https://support.rstudio.com/hc/en-us/articles/201141096-Getting-Started-with-R) and some of the links there\n- [LSE Digital Skills Lab resources](https://info.lse.ac.uk/current-students/digital-skills-lab/r)\n\n\n## Notes\n\nWe discussed Figure 1 from [The significance filter, the winner's curse and the need to shrink](https://arxiv.org/abs/2009.09440) and whether the [file-drawer effect](https://en.wikipedia.org/wiki/Publication_bias) helps to explain it.\n\n![](https://joshualoftus.com/posts/2020-12-21-concise-defense-of-statistical-significance/zscores.png)\n\n### Simulating many hypothesis tests\n\nWe created a simple simulation to understand how this might happen.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggplot2)\nlibrary(dplyr)\ntheme_set(theme_minimal())\nset.seed(1) # for reproducibility\n\n# Generate the simulated world\nN <- 5e4 # total hypotheses tested\nproportion_null <- .4\nsignif_level <- qnorm(.975)\nis_null <- rbinom(N, 1, proportion_null)\neffect_size_nonnull <- .5\nsimulated_world <- data.frame(is_null) |>\n  mutate(\n    zscore = rnorm(N, \n                   mean = (1 - is_null) * effect_size_nonnull,\n                   sd = 1 + .1 * (1 - is_null)))\nhead(simulated_world)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  is_null      zscore\n1       0  0.71621827\n2       0  0.03806304\n3       0  1.77959649\n4       1 -0.40575597\n5       0  1.31850857\n6       1  0.47661057\n```\n:::\n:::\n\n\nThis creates `zscores` with `mean = 0` and `sd = 1` under the null and larger `mean` and `sd` values when `is_null` is false.\n\n#### Observed effect sizes when proportion 0.4 are null\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsimulated_world |>\n  ggplot(aes(x = zscore, fill = factor(is_null))) +\n  geom_density(alpha = .5) +\n  geom_vline(xintercept = c(-1, 1) * signif_level,\n             linetype = \"dotted\") +\n  scale_fill_viridis_d(option = \"magma\")\n```\n\n::: {.cell-output-display}\n![](01-introduction_files/figure-html/plot_world-1.png){width=672}\n:::\n:::\n\n### Simulating publication bias\n\nBut analysts don't know which hypotheses are null, so they could not create this plot or separate the `zscore` values into the null and nonnull cases. Instead, some analysts may choose to only publish the results that seem significant.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Generate simulated published effects\nproportion_phack <- .9\nwhich_studies_phacked <- rbinom(N, 1, proportion_phack)\nsimulated_publications <-\n  simulated_world |>\n  mutate(phacked = which_studies_phacked) |>\n  dplyr::filter(phacked == 0 | # not p-hacked OR\n                abs(zscore) > signif_level) # large enough\nnrow(simulated_publications)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 8768\n```\n:::\n:::\n\n\n#### Published `zscores` when proportion 0.9 are p-hacked\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsimulated_publications |>\n  ggplot(aes(zscore)) +\n  geom_histogram(bins = 50)\n```\n\n::: {.cell-output-display}\n![](01-introduction_files/figure-html/plot_phack-1.png){width=672}\n:::\n:::\n",
    "supporting": [
      "01-introduction_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}