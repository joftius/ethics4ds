{
  "hash": "aa4b54b91bdb514430699625dfedbfe0",
  "result": {
    "markdown": "---\ntitle: \"Ethics4DS: Coursework 1\"\nauthor: \"[Candidate number here]\"\ndate: \"2022-11-01\"\noutput: html_document\n---\n\n\n\n\n## Discussion questions\n\n#### Data science is usually framed as utilitarian because of its focus on prediction/causation (consequences) and optimization (maximizing utility). Describe an example data science application using explicitly utilitarian language, then refer to at least one non-consequentialist theory to identify some aspect of the application that utilitarianism might overlook.\n\n- Example application: \n\n\n\n- A non-utilitarian aspect of this application:\n\n\n\n#### Choose one of the ethical data science guidelines that we read. Find some part of it that you agreed with strongly, quote that part, and describe why you thought it was important. Then find another part that you think is too limited, quote that part, and describe what you think is its most important limitation.\n\n- Guideline document: (choose one of ASA/RSS/ACM)\n\n- Agreement\n\n> quoted text\n\nReasoning: \n\n- Disagreement\n\n> quoted text\n\nReasoning: \n\n## Data questions\n\n### Computing fairness metrics\n\nUse the [fairness](https://kozodoi.me/r/fairness/packages/2020/05/01/fairness-tutorial.html) package. Pick one of the example datasets in the package. Fit a predictive model using that dataset. Choose three different fairness metrics to compute using the predictions from that model. For each of these, compute the values in the fairness metric in two ways: (1) using standard `R` functions, e.g. arithmetic operations, and (2) using the `fairness` package functions. Check to see whether you get the same answer.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# install.packages(\"fairness\")\nlibrary(fairness)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Predictive model\n```\n:::\n\n\n#### Fairness metric 1\n\nWhich metric: (name here)\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Computing manually\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Comparing to the fairness package answer\n```\n:::\n\n\n#### Fairness metric 2\n\nWhich metric: (name here)\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Computing manually\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Comparing to the fairness package answer\n```\n:::\n\n\n#### Fairness metric 3\n\nWhich metric: (name here)\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Computing manually\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Comparing to the fairness package answer\n```\n:::\n\n\n### Simulating a response variable\n\nNow replace the outcome variable in the original dataset with a new variable that you generate. You can decide how to generate the new outcome. Your goal is to make this outcome result in all the fairness metrics you chose above indicating that the predictive model is fair. \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# n <- nrow(datasetname)\n# datasetname$outcomename <- somefunction(n, etc)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Predictive model\n```\n:::\n\n\n#### Fairness metric 1\n\nWhich metric: (name here)\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Computing manually\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Comparing to the fairness package answer\n```\n:::\n\n\n#### Fairness metric 2\n\nWhich metric: (name here)\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Computing manually\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Comparing to the fairness package answer\n```\n:::\n\n\n#### Fairness metric 3\n\nWhich metric: (name here)\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Computing manually\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Comparing to the fairness package answer\n```\n:::\n\n\n#### Concluding thoughts\n\nDo any of the results above require some explanation? Briefly describe your conclusion here.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}