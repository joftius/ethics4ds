{
  "hash": "3c01a013f5d8155d7db05e792e50ae14",
  "result": {
    "markdown": "---\ntitle: \"Ethics4DS: Coursework 1\"\nauthor: \"[Candidate number here]\"\ndate: \"2023-10-29\"\noutput: html_document\n---\n\n\n\n\n## Reproducibility\n\nChange parameters of the simulation as necessary (or as desired, to make it unique)\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngenerate_data <- function(beta0 = 0,\n                          betaX = 0,\n                          betaA = 0,\n                          betaXA = 0,\n                          n = 200,\n                          proportion = 1/2) {\n  X <- rnorm(n)\n  A <- rbinom(n, 1, proportion)\n  Y <- beta0 + betaX * X + betaA * A + betaXA * X * A + rnorm(n, sd = 1)\n  data.frame(Y = Y, X = X, A = factor(A))\n}\n```\n:::\n\n\nHere is a visualisation to help understand the data\n\n\n::: {.cell}\n\n```{.r .cell-code}\none_sample <- generate_data(beta0 = 2, betaX = -1, betaA = 2, betaXA = 1.5)\none_sample |> \n  ggplot(aes(X, Y, color = A)) + \n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n`geom_smooth()` using formula = 'y ~ x'\n```\n:::\n\n::: {.cell-output-display}\n![](coursework1_answers_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\none_fit <- lm(Y ~ X + A + X * A, one_sample)\nsummary(one_fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = Y ~ X + A + X * A, data = one_sample)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.8553 -0.6802 -0.0024  0.6824  3.7687 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   1.9373     0.1103  17.560  < 2e-16 ***\nX            -0.8801     0.1258  -6.994  4.1e-11 ***\nA1            2.1303     0.1538  13.851  < 2e-16 ***\nX:A1          1.4017     0.1672   8.381  1.0e-14 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.086 on 196 degrees of freedom\nMultiple R-squared:  0.5808,\tAdjusted R-squared:  0.5744 \nF-statistic: 90.53 on 3 and 196 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(one_fit)$coefficients[,4]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n (Intercept)            X           A1         X:A1 \n4.291280e-42 4.101667e-11 7.199416e-31 1.004225e-14 \n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# p-values for all the non-intercept variables\nsummary(one_fit)$coefficients[-1,4]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n           X           A1         X:A1 \n4.101667e-11 7.199416e-31 1.004225e-14 \n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# p-value for second coeff\nsummary(one_fit)$coefficients[2,4]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 4.101667e-11\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Every time this function is called, it:\n# (1) generates new data\n# (2) fits a model using the formula Y ~ X + A + X * A\n# (3) returns the p-value for the second coefficient\none_experiment <- function(beta0 = 0, betaX = 0, betaA = 0,\n                           betaXA = 0, n = 200, proportion = 1/2) {\n  one_sample <- generate_data(beta0, betaX, betaA, betaXA, n, proportion)\n  one_fit <- lm(Y ~ X + A + X * A, one_sample)\n  p_values <- summary(one_fit)$coefficients[2,4]\n  return(p_values)\n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\none_experiment()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.8266538\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\none_experiment(betaX = .3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.1502811\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nn_experiments <- 5\nreplicate(n_experiments, one_experiment())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.9677839 0.5647648 0.1151294 0.2137763 0.7461851\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nrejection_level <- 0.05\nn_experiments <- 500\n# proportion of experiments where p-value\n# is below rejection level\nmean(replicate(n_experiments, one_experiment()) < rejection_level)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.032\n```\n:::\n:::\n\n\nConsider: What should this be? Is the above result acceptable or concerning?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmean(replicate(n_experiments, one_experiment(betaX = .2)) < rejection_level)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.474\n```\n:::\n:::\n\n\nA larger proportion of experiments had significant p-values this time. Is that what we expect?\n\nHow would this rejection rate depend on each of the parameters input to the `one_experiment()` function (e.g. `betaX`, `betaA`, `n`, etc)?\n\nWhen do we want the rejection rate to be high, and when do we want it to be low?\n\nNote: it may be helpful to review basic conditional control flow in R from sources like one of the following\n\n- [https://posit.cloud/learn/primers/6.5](https://posit.cloud/learn/primers/6.5)\n- [https://adv-r.hadley.nz/control-flow.html](https://adv-r.hadley.nz/control-flow.html) Section 5.2\n\n**Instructions**: after completing the rest of this notebook, delete this comment and all of the demonstration code above. You should only keep the code necessary for your answers below to work.\n\n### Preregistration vs p-hacking\n\n#### p-hacking \n\n- Modify the example code to create a function that simulates p-hacking\n- Show the effect of p-hacking on statistical error rates using any appropriate statistics or visualisations that you prefer\n- Explain, in your own words, what problems could result from p-hacking\n\n\n::: {.cell}\n\n```{.r .cell-code}\none_phacked_experiment <- function(beta0 = 0, betaX = 0, betaA = 0,\n                           betaXA = 0, n = 200, proportion = 1/2) {\n  one_sample <- generate_data(beta0, betaX, betaA, betaXA, n, proportion)\n  one_fit <- lm(Y ~ X + A + X * A, one_sample)\n  min_pvalue <- min(summary(one_fit)$coefficients[-1,4])\n  another_fit <- lm(Y ~ X, one_sample)\n  another_pvalue <- summary(another_fit)$coefficients[2,4]\n  p_values <- min(min_pvalue, another_pvalue)\n  return(p_values)\n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nrejection_level <- 0.05\nn_experiments <- 500\n# proportion of experiments where p-value\n# is below rejection level\nall_experiments_pvalues <- replicate(n_experiments, one_phacked_experiment())\nmean(all_experiments_pvalues < rejection_level)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.172\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nhist(all_experiments_pvalues)\n```\n\n::: {.cell-output-display}\n![](coursework1_answers_files/figure-html/unnamed-chunk-15-1.png){width=672}\n:::\n:::\n\n\n\n#### Preregistration\n\n- Create another version of the function to simulate the constraints of preregistration\n- Compare the error rates with preregistration to those without, and explain whether this could help with any problems you identified above and, if so, how\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\none_preregistered_experiment <- function(beta0 = 0, betaX = 0, betaA = 0,\n                           betaXA = 0, n = 200, proportion = 1/2) {\n  one_sample <- generate_data(beta0, betaX, betaA, betaXA, n, proportion)\n  one_fit <- lm(Y ~ X + A + X * A, one_sample)\n  several_pvalues <- summary(one_fit)$coefficients[-1,4]\n  another_fit <- lm(Y ~ X, one_sample)\n  another_pvalue <- summary(another_fit)$coefficients[2,4]\n  p_values <- c(several_pvalues, another_pvalue)\n  return(p_values)\n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nrejection_level <- 0.05\nn_experiments <- 500\n# proportion of experiments where p-value\n# is below rejection level\nall_experiments_pvalues <- replicate(n_experiments, one_preregistered_experiment())\nmean(all_experiments_pvalues < rejection_level)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.054\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nhist(all_experiments_pvalues)\n```\n\n::: {.cell-output-display}\n![](coursework1_answers_files/figure-html/unnamed-chunk-18-1.png){width=672}\n:::\n:::\n\n\n### Reproducibility and causality\n\n#### Reproducible and causal\n\n- Create another simulation, modifying the data generating code to be consistent with a specific causal model structure, and choosing the structure to satisfy the following the following requirements\n- Show that there is a reproducible effect, i.e. one that can be found fairly consistently (e.g. in more than five percent of experiments) without using p-hacking\n- Show, by simulating an intervention, that the above effect is causal\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# no changes necessary from original experiment\n# if true betaX is nonzero then rejections are not false discoveries\n# rejection rate:\nmean(replicate(n_experiments, one_experiment(betaX = .2)) < rejection_level)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.47\n```\n:::\n:::\n\n\nWhy is this effect causal?\n\nAssuming the code that generates the data accurately represents a causal model, we see the function generating Y depends on X. So if X is intervened on, then Y will also change. Here's a simple example showing the effect:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Using betaX = .2, all other coeffs = 0\nN <- 1000 # reduce sampling variation\n\n# Before intervention\nX <- rnorm(N)\nA <- rbinom(N, 1, 1/2)\nY <- 0.2 * X + rnorm(N, sd = 1)\n\n# After intervention\nX_new <- 2\nA_new <- rbinom(N, 1, 1/2)\nY_new <- 0.2 * X_new + rnorm(N, sd = 1)\n\nmean(Y_new) - mean(Y)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.4127188\n```\n:::\n:::\n\n\nThis shows that an intervention setting every value of X to 2 (for example) results in a different average value of Y\n\nConclusion: there is a true causal effect, and regression experiments find a statistically significant coefficient for X (with a rejection rate that is higher than 0.05)\n\n\n#### Reproducible but not causal\n\n- Repeat the above section, but in this case choose the causal model generating your data so that the reproducible effect is not causal, i.e. an intervention on that variable does not change the outcome variable\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngenerate_common_cause_data <- function(n = 100,\n                                    sdx = .2,\n                                    sdy = .2,\n                                    intervention = NULL) {\n  \n  # Original world\n  # Y <- U -> X\n  U <- rnorm(n) # unobserved\n  f_X <- function(U) U + 1 \n  f_Y <- function(U) (1/5) * (1 + U)^2\n  \n  X <- f_X(U) + rnorm(n, sd = sdx)\n  Y <- f_Y(U) + rnorm(n, sd = sdy)\n  \n  # Optional: intervene on X\n  if (!is.null(intervention)) {\n    # Intervened world\n    # Y <- U, X <- intervention\n    # U is generated before X\n    U <- rnorm(n)\n    # X is set to a specific value\n    X <- intervention\n    # Y is generated after X, but keeps its original\n    # distribution because f_Y does not use X\n    Y <- f_Y(U) + rnorm(n, sd = sdy)\n  }\n  \n  data.frame(Y = Y, X = X)\n}\n```\n:::\n\n\nOriginal data\n\n\n::: {.cell}\n\n```{.r .cell-code}\none_common_cause_dataset <- generate_common_cause_data(n = 400)\nggplot(one_common_cause_dataset, aes(X, Y)) + \n  geom_point(alpha = .5) +\n  geom_smooth(method = \"lm\", se = FALSE)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n`geom_smooth()` using formula = 'y ~ x'\n```\n:::\n\n::: {.cell-output-display}\n![](coursework1_answers_files/figure-html/unnamed-chunk-22-1.png){width=672}\n:::\n:::\n\n\nData with an intervention setting X to 1\n\n\n::: {.cell}\n\n```{.r .cell-code}\nintervened_common_cause_dataset <- \n  generate_common_cause_data(n = 400, intervention = 1)\nggplot(intervened_common_cause_dataset, aes(X, Y)) + \n  geom_point(alpha = .5) +\n  geom_smooth(method = \"lm\", se = FALSE)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n`geom_smooth()` using formula = 'y ~ x'\n```\n:::\n\n::: {.cell-output-display}\n![](coursework1_answers_files/figure-html/unnamed-chunk-23-1.png){width=672}\n:::\n:::\n\n\nAverages of Y before and after intervention\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmean(one_common_cause_dataset$Y)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.3960258\n```\n:::\n\n```{.r .cell-code}\nmean(intervened_common_cause_dataset$Y)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.4258304\n```\n:::\n:::\n\n\nSimulate one experiment\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Every time this function is called, it:\n# (1) generates new \"common cause\" data \n# (2) fits a model using the formula Y ~ X \n# (3) generates \"common cause\" data with intervention on X\n# (4) computes mean of Y before and after intervention\n# returns: X coefficient and p-value from (2), interventional effect (4)\none_common_cause_experiment <- function(n = 100,\n                                      sdx = .2,\n                                      sdy = .2,\n                                      intervention = 1) {\n  # without intervention\n  one_sample <- generate_common_cause_data(n, sdx, sdy)\n  one_fit <- lm(Y ~ X, one_sample)\n  beta_hat <- summary(one_fit)$coefficients[2,1]\n  p_value <- summary(one_fit)$coefficients[2,4]\n  \n  # with intervention\n  intervened_sample <- generate_common_cause_data(n, sdx, sdy, intervention)\n  avg_Y_diff <- mean(intervened_sample$Y) - mean(one_sample$Y)\n  return(list(beta_hat = beta_hat,\n              p_value = p_value,\n              intervention_effect = avg_Y_diff))\n}\n```\n:::\n\n\nSimulate multiple experiments and transform the results to make them easy to plot\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmultiple_common_cause_experiments <- function(n_experiments = 200,\n                                              n = 100,\n                                              sdx = .2,\n                                              sdy = .2,\n                                              intervention = 1) {\n  experiment_summary_list <- replicate(\n    n_experiments,\n    one_common_cause_experiment(n, sdx, sdy, intervention))\n  experiment_summaries <- apply(t(experiment_summary_list), 2, unlist)\n  return(data.frame(experiment_summaries))\n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nexperiment_summaries <- multiple_common_cause_experiments()\n```\n:::\n\n\nDistribution of estimated coefficients for X\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(experiment_summaries, aes(beta_hat)) + geom_histogram(bins = 20)\n```\n\n::: {.cell-output-display}\n![](coursework1_answers_files/figure-html/unnamed-chunk-28-1.png){width=672}\n:::\n:::\n\n\n\nRejection rate using p-values for X\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrejection_level <- 0.05\n# proportion of experiments where p-value\n# is below rejection level\nmean(experiment_summaries$p_value < rejection_level)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1\n```\n:::\n:::\n\n\nDistribution of mean difference in Y when intervening on X\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(experiment_summaries, aes(intervention_effect)) + \n  geom_histogram(bins = 20)\n```\n\n::: {.cell-output-display}\n![](coursework1_answers_files/figure-html/unnamed-chunk-30-1.png){width=672}\n:::\n:::\n\n\nAverage interventional effect across multiple experiments (meta-analysis)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmean(experiment_summaries$intervention_effect)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] -0.01083109\n```\n:::\n:::\n\n\nConclusion: experiments consistently find a statistically significant coefficient for X (**without** using p-hacking to achieve significance), and yet there is no causal effect of X on Y (their significant association is due to having a common cause U, which in our simulation is unobserved by the experimenter)\n\n\n",
    "supporting": [
      "coursework1_answers_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}