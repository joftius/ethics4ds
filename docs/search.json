[
  {
    "objectID": "weeks/01-introduction.html",
    "href": "weeks/01-introduction.html",
    "title": "Introduction and examples",
    "section": "",
    "text": "We begin by considering examples within two broad themes: the replication crisis in science and fairness and inequality in algorithmic or data-driven systems."
  },
  {
    "objectID": "weeks/01-introduction.html#assigned-readings",
    "href": "weeks/01-introduction.html#assigned-readings",
    "title": "Introduction, normative ethics, and causality",
    "section": "Assigned readings",
    "text": "Assigned readings\n\nBook chapter"
  },
  {
    "objectID": "weeks/01-introduction.html#additional-references",
    "href": "weeks/01-introduction.html#additional-references",
    "title": "Introduction, normative ethics, and causality",
    "section": "Additional references",
    "text": "Additional references\n\nReplication crisis\n\nWhy Most Published Research Findings Are False\nMany Analysts, One Dataset\nEstimating the reproducibility of psychological science\nLikelihood of Null Effects of Large NHLBI Clinical Trials Has Increased over Time\nSurvey on Reproducibility Crisis\n\n\n\n(Un)fair algorithms\n\nRedlining, Amazon’s same day delivery, and car insurance premiums\nGuardian series on automating poverty\nRacial bias in personalized medicine\n\n\n\nPhilosophy / normative ethics\n\nWikipedia\nPodcast interview with William MacAskill\nBlog post on ethics and politics of AI\n\n\n\nGetting started with R\n\nRStudio blog post and some of the links there\nLSE Digital Skills Lab resources"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Weekly content",
    "section": "",
    "text": "Nov 11, 2022\n\n\nJoshua Loftus\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOct 31, 2022\n\n\nJoshua Loftus\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOct 22, 2022\n\n\nJoshua Loftus\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOct 16, 2022\n\n\nJoshua Loftus\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOct 10, 2022\n\n\nJoshua Loftus\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOct 3, 2022\n\n\nJoshua Loftus\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSep 26, 2022\n\n\nJoshua Loftus\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Course information",
    "section": "",
    "text": "Course guide listing"
  },
  {
    "objectID": "weeks/01-introduction.html#installing-r-and-rstudio",
    "href": "weeks/01-introduction.html#installing-r-and-rstudio",
    "title": "Introduction, normative ethics, and causality",
    "section": "Installing R and RStudio",
    "text": "Installing R and RStudio\nFirst install R and then install RStudio (this second step is highly recommended but not required, if you prefer another IDE and you’re sure you know what you’re doing). Finally, open RStudio and install the tidyverse set of packages by running the command\ninstall.packages(\"tidyverse\")\nNote: If you use a Mac or Linux-based computer you may want to install these using a package manager instead of downloading them from the websites linked above. Personally, on a Mac computer I use Homebrew (the link has instructions for how to install it) to install R and RStudio."
  },
  {
    "objectID": "weeks/01-introduction.html#references",
    "href": "weeks/01-introduction.html#references",
    "title": "Introduction and examples",
    "section": "References",
    "text": "References\n\nAssigned reading\n\nBook chapter\nWikipedia: Replication crisis\n\n\n\nAdditional references\n\nReplication crisis\n\nWhy Most Published Research Findings Are False\nMany Analysts, One Dataset\nEstimating the reproducibility of psychological science\nLikelihood of Null Effects of Large NHLBI Clinical Trials Has Increased over Time\nSurvey on Reproducibility Crisis\np-Hacking in A/B Testing\n\n\n\n(Un)fair algorithms\n\nFairML book Chapter 1: Introduction\nRedlining, Amazon’s same day delivery, and car insurance premiums\nGuardian series on automating poverty\nRacial bias in personalized medicine"
  },
  {
    "objectID": "weeks/01-introduction.html#computer-setup",
    "href": "weeks/01-introduction.html#computer-setup",
    "title": "Introduction and examples",
    "section": "Computer setup",
    "text": "Computer setup\n\nInstalling R and RStudio\nFirst install R and then install RStudio (this second step is highly recommended but not required, if you prefer another IDE and you’re sure you know what you’re doing). Finally, open RStudio and install the tidyverse set of packages by running the command\ninstall.packages(\"tidyverse\")\nNote: If you use a Mac or Linux-based computer you may want to install these using a package manager instead of downloading them from the websites linked above. Personally, on a Mac computer I use Homebrew (the link has instructions for how to install it) to install R and RStudio.\n\n\nResources for learning R\n\nRStudio blog post and some of the links there\nLSE Digital Skills Lab resources"
  },
  {
    "objectID": "weeks/01-introduction.html#notes",
    "href": "weeks/01-introduction.html#notes",
    "title": "Introduction and examples",
    "section": "Notes",
    "text": "Notes\nWe discussed Figure 1 from The significance filter, the winner’s curse and the need to shrink and whether the file-drawer effect helps to explain it.\n\n\nSimulating many hypothesis tests\nWe created a simple simulation to understand how this might happen.\n\nlibrary(ggplot2)\nlibrary(dplyr)\ntheme_set(theme_minimal())\nset.seed(1) # for reproducibility\n\n# Generate the simulated world\nN <- 5e4 # total hypotheses tested\nproportion_null <- .4\nsignif_level <- qnorm(.975)\nis_null <- rbinom(N, 1, proportion_null)\neffect_size_nonnull <- .5\nsimulated_world <- data.frame(is_null) |>\n  mutate(\n    zscore = rnorm(N, \n                   mean = (1 - is_null) * effect_size_nonnull,\n                   sd = 1 + .1 * (1 - is_null)))\nhead(simulated_world)\n\n  is_null      zscore\n1       0  0.71621827\n2       0  0.03806304\n3       0  1.77959649\n4       1 -0.40575597\n5       0  1.31850857\n6       1  0.47661057\n\n\nThis creates zscores with mean = 0 and sd = 1 under the null and larger mean and sd values when is_null is false.\n\nObserved effect sizes when proportion 0.4 are null\n\nsimulated_world |>\n  ggplot(aes(x = zscore, fill = factor(is_null))) +\n  geom_density(alpha = .5) +\n  geom_vline(xintercept = c(-1, 1) * signif_level,\n             linetype = \"dotted\") +\n  scale_fill_viridis_d(option = \"magma\")\n\n\n\n\n\n\n\nSimulating publication bias\nBut analysts don’t know which hypotheses are null, so they could not create this plot or separate the zscore values into the null and nonnull cases. Instead, some analysts may choose to only publish the results that seem significant.\n\n# Generate simulated published effects\nproportion_phack <- .9\nwhich_studies_phacked <- rbinom(N, 1, proportion_phack)\nsimulated_publications <-\n  simulated_world |>\n  mutate(phacked = which_studies_phacked) |>\n  dplyr::filter(phacked == 0 | # not p-hacked OR\n                abs(zscore) > signif_level) # large enough\nnrow(simulated_publications)\n\n[1] 8768\n\n\n\nPublished zscores when proportion 0.9 are p-hacked\n\nsimulated_publications |>\n  ggplot(aes(zscore)) +\n  geom_histogram(bins = 50)"
  },
  {
    "objectID": "weeks/02-research.html",
    "href": "weeks/02-research.html",
    "title": "Ethical research conduct",
    "section": "",
    "text": "Historical examples of harmful research, guidelines for ethical research, reproducible practices."
  },
  {
    "objectID": "weeks/02-research.html#references",
    "href": "weeks/02-research.html#references",
    "title": "Ethical research conduct",
    "section": "References",
    "text": "References\n\nAssigned reading\n\nBook chapter continues as a broad overview\nThe Role of the Statistician: Scientist or Shoe Clerk by Irwin D. T. Bross\nWikipedia: Tuskegee Syphilis Study\nWikipedia: Belmont Report\nWikipedia: History of Health_effects_of_tobacco\n\nWikipedia pages can be briefly skimmed.\n\n\nAdditional references\n\nStatistical pluralism\n\nStatistical Modeling: The Two Cultures by Leo Breiman (and comments by others)\nConclusions vs Decisions by John W. Tukey\nStatistical Criticism by Bross (and responses by others)\n\n\n\nGood researchers\n\nBill Jenkins\n\n\n\nResearch hall of shame\n\nSir Ronald A. Fisher FRS (see this history lecture and stats about smoking)\nZimbardo, etc\nWansink, etc\n\n\n\nGuidelines"
  },
  {
    "objectID": "weeks/02-research.html#notes",
    "href": "weeks/02-research.html#notes",
    "title": "Ethical research conduct",
    "section": "Notes",
    "text": "Notes"
  },
  {
    "objectID": "weeks/03-research.html",
    "href": "weeks/03-research.html",
    "title": "Ethical research conduct",
    "section": "",
    "text": "Historical examples of harmful research, guidelines for ethical research, reproducible practices."
  },
  {
    "objectID": "weeks/03-research.html#references",
    "href": "weeks/03-research.html#references",
    "title": "Ethical research conduct",
    "section": "References",
    "text": "References\n\nAssigned reading\n\nAt least two from the following ethical guidelines created by professional societies:\n\nRoyal Statistical Society\nAmerican Statistical Association\nAssociation for Computing Machinery\n\n\nWikipedia pages to briefly skim:\n\nTuskegee Syphilis Study (read summary section at the top)\nBelmont Report (read History section)\nMenlo Report (read Implementation of the Principles section)\n\n\n\nAdditional references\n\nGood researchers\n\nBill Jenkins"
  },
  {
    "objectID": "weeks/03-research.html#notes",
    "href": "weeks/03-research.html#notes",
    "title": "Ethical research conduct",
    "section": "Notes",
    "text": "Notes"
  },
  {
    "objectID": "weeks/02-history.html",
    "href": "weeks/02-history.html",
    "title": "What do we want from Statistics?",
    "section": "",
    "text": "Vignettes about the history, philosophy, and profession of statistics."
  },
  {
    "objectID": "weeks/02-history.html#references",
    "href": "weeks/02-history.html#references",
    "title": "What do we want from Statistics?",
    "section": "References",
    "text": "References\n\nAssigned reading\n\nA Day in the Life of a Statistician (brief sketch written by a historian of science)\nWikipedia: one, two, three, four articles to briefly scan\nAt least one of:\n\nSurrogate Science: The Idol of a Universal Method for Scientific Inference\nCargo-cult statistics and scientific crisis\n\nAt least one of:\n\nThe Role of the Statistician: Scientist or Shoe Clerk\nStatistical Criticism Sections 1-5\n\n\nWikipedia pages can be briefly skimmed.\n\n\nAdditional references\n\nStatistical pluralism\n\nStatistical Modeling: The Two Cultures by Leo Breiman (and comments by others)\nConclusions vs Decisions by John W. Tukey\nStatistical Criticism with commentary\n\n\n\nResearch hall of shame\n\nZimbardo (note: his response to criticisms), etc\nWansink, etc"
  },
  {
    "objectID": "weeks/02-history.html#notes",
    "href": "weeks/02-history.html#notes",
    "title": "What do we want from Statistics?",
    "section": "Notes",
    "text": "Notes\n\nExcerpts from The Pernicious Influence of Mathematics on Science\n\nI wish to confine myself to the negative aspects, leaving it to others to dwell on the amazing triumphs of the mathematical method; and also to comment not only on physical science but also on social science, in which the characteristic inadequacies which I wish to discuss are more readily apparent.\nComputer programmers often make a certain remark about computing machines, which may perhaps be taken as a complaint: that computing machines, with a perfect lack of discrimination, will do any foolish thing they are told to do. The reason for this lies of course in the narrow fixation of the computing machine “intelligence” upon the basely typographical details of its own perceptions–its inability to be guided by any large context. In a psychological description of the computer intelligence, three related adjectives push themselves forward: single-mindedness, literal-mindedness, simple- mindedness. Recognizing this, we should at the same time recognize that this single-mindedness, literal-mindedness, simple-mindedness also characterizes theoretical mathematics, though to a lesser extent.\nIt is a continual result of the fact that science tries to deal with reality that even the most precise sciences normally work with more or less ill-understood approximations toward which the scientist must maintain an appropriate skepticism. […] This very healthy self-skepticism is foreign to the mathematical approach.\n[…] The mathematician turns the scientist’s theoretical assumptions, i.e., convenient points of analytical emphasis, into axioms, and then takes these axioms literally. This brings with it the danger that he may also persuade the scientist to take these axioms literally. The question, central to the scientific investigation but intensely disturbing in the mathematical context–what happens to all this if the axioms are relaxed–is thereby put into shadow.\n[…] Related to this deficiency of mathematics, and perhaps more productive of rueful consequence, is the simple-mindedness of mathematics–its willingness, like that of a computing machine, to elaborate upon any idea, however absurd; to dress scientific brilliancies and scientific absurdities alike in the impressive uniform of formulae and theorems. Unfortunately however, an absurdity in uniform is far more persuasive than an absurdity unclad. The very fact that a theory appears in mathematical form, that, for instance, a theory has provided the occasion for the application of a fixed-point theorem, or of a result about difference equations, somehow makes us more ready to take it seriously. And the mathematical-intellectual effort of applying the theorem fixes in us the particular point of view of the theory with which we deal, making us blind to whatever appears neither as a dependent nor as an independent parameter in its mathematical formulation.\nThe result, perhaps most common in the social sciences, is bad theory with a mathematical passport. […]\n\n\n\nExcerpts from Cargo Cult Science html version\n\n[…] the educational and psychological studies I mentioned are examples of what I would like to call Cargo Cult Science. In the South Seas there is a Cargo Cult of people. During the war they saw airplanes land with lots of good materials, and they want the same thing to happen now. So they’ve arranged to make things like runways, to put fires along the sides of the runways, to make a wooden hut for a man to sit in, with two wooden pieces on his head like headphones and bars of bamboo sticking out like antennas—he’s the controller—and they wait for the airplanes to land. They’re doing everything right. The form is perfect. It looks exactly the way it looked before. But it doesn’t work. No airplanes land. So I call these things Cargo Cult Science, because they follow all the apparent precepts and forms of scientific investigation, but they’re missing something essential, because the planes don’t land.\nNow it behooves me, of course, to tell you what they’re missing. But it would he just about as difficult to explain to the South Sea Islanders how they have to arrange things so that they get some wealth in their system. It is not something simple like telling them how to improve the shapes of the earphones. But there is one feature I notice that is generally missing in Cargo Cult Science. That is the idea that we all hope you have learned in studying science in school—we never explicitly say what this is, but just hope that you catch on by all the examples of scientific investigation. It is interesting, therefore, to bring it out now and speak of it explicitly. It’s a kind of scientific integrity, a principle of scientific thought that corresponds to a kind of utter honesty—a kind of leaning over backwards. For example, if you’re doing an experiment, you should report everything that you think might make it invalid—not only what you think is right about it: other causes that could possibly explain your results; and things you thought of that you’ve eliminated by some other experiment, and how they worked—to make sure the other fellow can tell they have been eliminated.\nDetails that could throw doubt on your interpretation must be given, if you know them. You must do the best you can—if you know anything at all wrong, or possibly wrong—to explain it. If you make a theory, for example, and advertise it, or put it out, then you must also put down all the facts that disagree with it, as well as those that agree with it. There is also a more subtle problem. When you have put a lot of ideas together to make an elaborate theory, you want to make sure, when explaining what it fits, that those things it fits are not just the things that gave you the idea for the theory; but that the finished theory makes something else come out right, in addition.\nIn summary, the idea is to try to give all of the information to help others to judge the value of your contribution; not just the information that leads to judgment in one particular direction or another.\nThe easiest way to explain this idea is to contrast it, for example, with advertising. […]\nWe’ve learned from experience that the truth will out. Other experimenters will repeat your experiment and find out whether you were wrong or right. Nature’s phenomena will agree or they’ll disagree with your theory. And, although you may gain some temporary fame and excitement, you will not gain a good reputation as a scientist if you haven’t tried to be very careful in this kind of work. And it’s this type of integrity, this kind of care not to fool yourself, that is missing to a large extent in much of the research in Cargo Cult Science.\nA great deal of their difficulty is, of course, the difficulty of the subject and the inapplicability of the scientific method to the subject. Nevertheless, it should be remarked that this is not the only difficulty. That’s why the planes don’t land—but they don’t land.\nWe have learned a lot from experience about how to handle some of the ways we fool ourselves. […]\nBut this long history of learning how to not fool ourselves—of having utter scientific integrity—is, I’m sorry to say, something that we haven’t specifically included in any particular course that I know of. We just hope you’ve caught on by osmosis.\nThe first principle is that you must not fool yourself—and you are the easiest person to fool. So you have to be very careful about that. After you’ve not fooled yourself, it’s easy not to fool other scientists. You just have to be honest in a conventional way after that.\n[…]\nOne example of the principle is this: If you’ve made up your mind to test a theory, or you want to explain some idea, you should always decide to publish it whichever way it comes out. If we only publish results of a certain kind, we can make the argument look good. We must publish both kinds of result. For example—let’s take advertising again—suppose some particular cigarette has some particular property, like low nicotine. It’s published widely by the company that this means it is good for you—they don’t say, for instance, that the tars are a different proportion, or that something else is the matter with the cigarette. In other words, publication probability depends upon the answer. That should not be done.\nI say that’s also important in giving certain types of government advice. Supposing a senator asked you for advice about whether drilling a hole should be done in his state; and you decide it would he better in some other state. If you don’t publish such a result, it seems to me you’re not giving scientific advice. You’re being used. If your answer happens to come out in the direction the government or the politicians like, they can use it as an argument in their favor; if it comes out the other way, they don’t publish it at all. That’s not giving scientific advice.\n[…] I was shocked to hear of an experiment done at the big accelerator at the National Accelerator Laboratory, where a person used deuterium. In order to compare his heavy hydrogen results to what might happen to light hydrogen he had to use data from someone else’s experiment on light hydrogen, which was done on different apparatus. When asked he said it was because he couldn’t get time on the program (because there’s so little time and it’s such expensive apparatus) to do the experiment with light hydrogen on this apparatus because there wouldn’t be any new result. And so the men in charge of programs at NAL are so anxious for new results, in order to get more money to keep the thing going for public relations purposes, they are destroying—possibly—the value of the experiments themselves, which is the whole purpose of the thing. It is often hard for the experimenters there to complete their work as their scientific integrity demands.\nAll experiments in psychology are not of this type, however. For example, there have been many experiments running rats through all kinds of mazes, and so on—with little clear result. But in 1937 a man named Young did a very interesting one. He had a long corridor with doors all along one side where the rats came in, and doors along the other side where the food was. He wanted to see if he could train the rats to go in at the third door down from wherever he started them off. No. The rats went immediately to the door where the food had been the time before.\nThe question was, how did the rats know, because the corridor was so beautifully built and so uniform, that this was the same door as before? Obviously there was something about the door that was different from the other doors. So he painted the doors very carefully, arranging the textures on the faces of the doors exactly the same. Still the rats could tell. Then he thought maybe the rats were smelling the food, so he used chemicals to change the smell after each run. Still the rats could tell. Then he realized the rats might be able to tell by seeing the lights and the arrangement in the laboratory like any commonsense person. So he covered the corridor, and, still the rats could tell.\nHe finally found that they could tell by the way the floor sounded when they ran over it. And he could only fix that by putting his corridor in sand. So he covered one after another of all possible clues and finally was able to fool the rats so that they had to learn to go in the third door. If he relaxed any of his conditions, the rats could tell.\nNow, from a scientific standpoint, that is an A‑Number‑l experiment. That is the experiment that makes rat‑running experiments sensible, because it uncovers the clues that the rat is really using—not what you think it’s using. And that is the experiment that tells exactly what conditions you have to use in order to be careful and control everything in an experiment with rat‑running.\nI looked into the subsequent history of this research. The subsequent experiment, and the one after that, never referred to Mr. Young. They never used any of his criteria of putting the corridor on sand, or being very careful. They just went right on running rats in the same old way, and paid no attention to the great discoveries of Mr. Young, and his papers are not referred to, because he didn’t discover anything about the rats. In fact, he discovered all the things you have to do to discover something about rats. But not paying attention to experiments like that is a characteristic of Cargo Cult Science.\nAnother example is the ESP experiments of Mr. Rhine, and other people. As various people have made criticisms—and they themselves have made criticisms of their own experiments—they improve the techniques so that the effects are smaller, and smaller, and smaller until they gradually disappear. All the parapsychologists are looking for some experiment that can be repeated—that you can do again and get the same effect—statistically, even. They run a million rats—no, it’s people this time—they do a lot of things and get a certain statistical effect. Next time they try it they don’t get it any more. And now you find a man saying that it is an irrelevant demand to expect a repeatable experiment. This is science?\nThis man also speaks about a new institution, in a talk in which he was resigning as Director of the Institute of Parapsychology. And, in telling people what to do next, he says that one of the things they have to do is be sure they only train students who have shown their ability to get PSI results to an acceptable extent—not to waste their time on those ambitious and interested students who get only chance results. It is very dangerous to have such a policy in teaching—to teach students only how to get certain results, rather than how to do an experiment with scientific integrity.\nSo I wish to you—I have no more time, so I have just one wish for you—the good luck to be somewhere where you are free to maintain the kind of integrity I have described, and where you do not feel forced by a need to maintain your position in the organization, or financial support, or so on, to lose your integrity. May you have that freedom. […]"
  },
  {
    "objectID": "weeks/04-fairness.html",
    "href": "weeks/04-fairness.html",
    "title": "Fairness and prediction",
    "section": "",
    "text": "The data and formal precision of predictive algorithms make evident certain kinds of unfairness. There are many ideas about how to make algorithms more fair, and this week we will consider a few examples of statistical definitions of fairness."
  },
  {
    "objectID": "weeks/04-fairness.html#references",
    "href": "weeks/04-fairness.html#references",
    "title": "Fairness and prediction",
    "section": "References",
    "text": "References\n\nAssigned reading\n\nFairML Book Chapter 3 on Classification. It’s OK to skip the proofs in the section called Relationships between criteria.\nProPublica Machine Bias\n\n\n\nAdditional references\n\nProPublica COMPAS Methods Article\n50 Years of Test (Un)fairness: Lessons for Machine Learning"
  },
  {
    "objectID": "weeks/04-fairness.html#notes",
    "href": "weeks/04-fairness.html#notes",
    "title": "Fairness and prediction",
    "section": "Notes",
    "text": "Notes"
  },
  {
    "objectID": "weeks/05-opportunity.html",
    "href": "weeks/05-opportunity.html",
    "title": "Motivations for types of fairness",
    "section": "",
    "text": "The data and formal precision of predictive algorithms make evident certain kinds of unfairness. There are many ideas about how to make algorithms more fair, and this week we will consider a few examples of statistical definitions of fairness."
  },
  {
    "objectID": "weeks/05-opportunity.html#references",
    "href": "weeks/05-opportunity.html#references",
    "title": "Motivations for types of fairness",
    "section": "References",
    "text": "References\n\nAssigned reading\n\nInherent Trade-Offs in the Fair Determination of Risk Scores read Section 1 through the statement of Theorem 1.1, and also read the Conclusion (Section 5).\nFairML Book Chapter 4 on Relative notions of fairness, beginning with the section on Intentionality and indirect discrimination.\n\n\n\nAdditional references\n\nFair prediction with disparate impact: A study of bias in recidivism prediction instruments\nFairML Book the rest of Chapter 4."
  },
  {
    "objectID": "weeks/05-opportunity.html#notes",
    "href": "weeks/05-opportunity.html#notes",
    "title": "Motivations for types of fairness",
    "section": "Notes",
    "text": "Notes"
  },
  {
    "objectID": "pset1.html",
    "href": "pset1.html",
    "title": "Ethics4DS: Coursework 1",
    "section": "",
    "text": "Example application:\nA non-utilitarian aspect of this application:\n\n\n\n\n\nGuideline document: (choose one of ASA/RSS/ACM)\nAgreement\n\n\nquoted text\n\nReasoning:\n\nDisagreement\n\n\nquoted text\n\nReasoning:"
  },
  {
    "objectID": "pset1.html#data-questions",
    "href": "pset1.html#data-questions",
    "title": "Ethics4DS: Coursework 1",
    "section": "Data questions",
    "text": "Data questions\n\nComputing fairness metrics\nUse the fairness package. Pick one of the example datasets in the package. Fit a predictive model using that dataset. Choose three different fairness metrics to compute using the predictions from that model. For each of these, compute the values in the fairness metric in two ways: (1) using standard R functions, e.g. arithmetic operations, and (2) using the fairness package functions. Check to see whether you get the same answer.\n\n# install.packages(\"fairness\")\nlibrary(fairness)\n\n\n# Predictive model\n\n\nFairness metric 1\nWhich metric: (name here)\n\n# Computing manually\n\n\n# Comparing to the fairness package answer\n\n\n\nFairness metric 2\nWhich metric: (name here)\n\n# Computing manually\n\n\n# Comparing to the fairness package answer\n\n\n\nFairness metric 3\nWhich metric: (name here)\n\n# Computing manually\n\n\n# Comparing to the fairness package answer\n\n\n\n\nSimulating a response variable\nNow replace the outcome variable in the original dataset with a new variable that you generate. You can decide how to generate the new outcome. Your goal is to make this outcome result in all the fairness metrics you chose above indicating that the predictive model is fair.\n\n# n <- nrow(datasetname)\n# datasetname$outcomename <- somefunction(n, etc)\n\n\n# Predictive model\n\n\nFairness metric 1\nWhich metric: (name here)\n\n# Computing manually\n\n\n# Comparing to the fairness package answer\n\n\n\nFairness metric 2\nWhich metric: (name here)\n\n# Computing manually\n\n\n# Comparing to the fairness package answer\n\n\n\nFairness metric 3\nWhich metric: (name here)\n\n# Computing manually\n\n\n# Comparing to the fairness package answer\n\n\n\nConcluding thoughts\nDo any of the results above require some explanation? Briefly describe your conclusion here."
  },
  {
    "objectID": "weeks/06-causality.html",
    "href": "weeks/06-causality.html",
    "title": "Causal models",
    "section": "",
    "text": "Causal models are useful for understanding different kinds of statistical relationships between two or more variables."
  },
  {
    "objectID": "weeks/06-causality.html#references",
    "href": "weeks/06-causality.html#references",
    "title": "Causal models",
    "section": "References",
    "text": "References\n\nAssigned reading\n\nFairML Book Chapter 5 on Causality, at least to the section on Counterfactuals (about halfway through).\n\n\n\nAdditional references\n\nFairML Book the rest of Chapter 5.\nCausal Inference: The Mixtape Sections 1.1, 1.2, and 3.1 up to 3.1.3 (stop before 3.1.4)"
  },
  {
    "objectID": "weeks/06-causality.html#notes",
    "href": "weeks/06-causality.html#notes",
    "title": "Causal models",
    "section": "Notes",
    "text": "Notes\n\nSimulating data based on DAGs\nNote: we can make nice graphs using the ggdag package to plot graphs.\n\nGraph: \\(X \\rightarrow Y\\)\nOriginal world\n\nlibrary(tidyverse)\ntheme_set(theme_minimal())\nset.seed(1)\nn <- 200\n# Step 1: Generate variables with no parents\nf_X <- function(noise) noise - 1\nnoise_X <- rnorm(n)\nX <- f_X(noise_X)\n# Step k+1: Generate variables that had their\n# set of parents finish generating at step k\nf_Y <- function(x, noise) 3 * x + 2 * noise\nnoise_Y <- rnorm(n)\nY <- f_Y(X, noise_Y)\nqplot(X, Y)\n\n\n\n\nSample average of Y:\n\nmean(Y)\n\n[1] -2.812106\n\n\nWorld after intervention\nWe start by copying and pasting the original code, then we modify the program to change some variable. In this case we do an “atomic” intervention setting all \\(X\\) values to 1.\nSince the code is written in a way that any variables depending on \\(X\\) (in this graph \\(Y\\) does) are generated after \\(X\\), this intervention on \\(X\\) may change their distributions as well.\n\n# Step 1: Generate variables with no parents\nX <- 1\n# Step k+1: Generate variables that had their\n# set of parents finish generating at step k\nf_Y <- function(x, noise) 3 * x + 2 * noise\nnoise_Y <- rnorm(n)\nY <- f_Y(X, noise_Y)\nqplot(X, Y)\n\n\n\n\nSample average of Y:\n\nmean(Y)\n\n[1] 2.916346\n\n\nExplanation\nWith this simple data generating process we can see that \\(X \\sim N(-1, 1)\\) and \\((Y | X = x) \\sim N(3x, 4)\\). By linearity, \\(E[Y] = 3E[X] = -3\\) in the original world. But after the intervention \\(\\text{do}(X := 1)\\), we have \\(E[Y] = 3 E[1] = 3 \\cdot 1 = 3\\).\n\n\nGraph: \\(X \\leftarrow U \\rightarrow Y\\)\nOriginal world\n\nn <- 10000 # reduce sampling variability\n# Step 1: Generate variables with no parents\nU <- rnorm(n)\n# Step k+1: Generate variables that had their\n# set of parents finish generating at step k\nf_X <- function(u, noise) 2 * u + 3 + noise\nnoise_X <- rnorm(n)\nX <- f_X(U, noise_X)\nf_Y <- function(u, noise) u^2 + noise^2\nnoise_Y <- rnorm(n)\nY <- f_Y(U, noise_Y)\n\nSample average of Y:\n\nmean(Y)\n\n[1] 2.030961\n\n\nWorld after intervention\nAn “atomic” intervention setting all X values to 1.\n\n# Step 1: Generate variables with no parents\nU <- rnorm(n)\n# Step k+1: Generate variables that had their\n# set of parents finish generating at step k\nX <- 1\nf_Y <- function(u, noise) u^2 + noise^2\nnoise_Y <- rnorm(n)\nY <- f_Y(U, noise_Y)\n\nSample average of Y:\n\nmean(Y)\n\n[1] 2.032377\n\n\nExplanation\nIn this case the mean of \\(Y\\) did not change because the variable we intervened on, \\(X\\), is not a cause of \\(Y\\)."
  },
  {
    "objectID": "weeks/07-causaltools.html",
    "href": "weeks/07-causaltools.html",
    "title": "Applying causality",
    "section": "",
    "text": "Applying causal models to analyze questions in algorithmic fairness and scientific reproducibility."
  },
  {
    "objectID": "weeks/07-causaltools.html#references",
    "href": "weeks/07-causaltools.html#references",
    "title": "Applying causality",
    "section": "References",
    "text": "References\n\nAssigned reading\n\nFairML Book the rest of Chapter 5 on Causality.\n\n\n\nAdditional references\n\nDisparate Causes\nCausal Inference: The Mixtape Sections 1.1, 1.2, and 3.1 up to 3.1.3 (stop before 3.1.4)"
  },
  {
    "objectID": "weeks/07-causaltools.html#notes",
    "href": "weeks/07-causaltools.html#notes",
    "title": "Applying causality",
    "section": "Notes",
    "text": "Notes"
  },
  {
    "objectID": "answers1.html",
    "href": "answers1.html",
    "title": "Ethics4DS: Coursework 1 answers",
    "section": "",
    "text": "Data science is usually framed as utilitarian because of its focus on prediction/causation (consequences) and optimization (maximizing utility). Describe an example data science application using explicitly utilitarian language, then refer to at least one non-consequentialist theory to identify some aspect of the application that utilitarianism might overlook.\nExample application\n(This answer is written somewhat abstractly in order to include many possible applications)\nA common kind of application is for an organization to predict “risk” for individuals and make decisions based on these predictions. These decisions usually involve allocating some kind of resource. Decision makers at the organizations usually believe allocating resources when the predicted risk is lower will achieve better outcomes (i.e. higher utility) for the organization, and they probably also believe the organization where they work makes a positive contribution to society and therefore to total utility.\n\nA tech company may predict users’ interest in different kinds of content to decide which items to recommend (for viewing/purchasing) or which advertisements to show\nA university may predict academic performance of applicants to decide which students to admit\nA bank may predict credit scores to decide how much credit to make available to different customers\nA government agency may predict threats to public health or safety, like outbreaks of viruses or violence, and decide who/where to monitor more closely\n\nSeveral utilitarian criticisms:\n\nSocial choice problem: the utility of the organization may align poorly with the total utility of society\nData science problem: predictions of risk are usually based on correlations/associations and hence may not capture causal relationships, or the measures of risk may be poor proxies for an unobserved variable that would better inform decisions. Hence allocating resources based on predicted risk may not actually maximize (or even increase) the organizations’ (or society’s) utility\n\nA non-utilitarian aspect of this application\nOne possible deontological issue: there may be laws or ethical duties requiring an organization does not make decisions about individuals based on certain “protected” characteristics or variables. But many of the same variables that are commonly used to predict risk can be associated with protected variables, so predictive models could end up using information about protected variables either explicitly or implicitly. Deontological ethics may encourage us to remove such signals from the risk predictions even if the consequences result in lower utility for the organization.\nOne possible virtue ethics issue: the utility of the organization may align poorly with certain virtues. These virtues could become neglected if people focus narrowly on whatever other traits they believe are beneficial to the organization’s risk predictions or decisions.\n\n\n\nChoose one of the ethical data science guidelines that we read. Find some part of it that you agreed with strongly, quote that part, and describe why you thought it was important. Then find another part that you think is too limited, quote that part, and describe what you think is its most important limitation.\n\nGuideline document: ASA\nAgreement\n\nFrom Section A:\n\nThe ethical statistical practitioner: … Does not knowingly conduct statistical practices that exploit vulnerable populations or create or perpetuate unfair outcomes.\n\nReasoning: I’m most impressed by the idea that we should not perpetuate (or continue) some existing unfairness. There are good reasons why avoiding harm is often stated as a first ethical principle, and avoiding harm could correspond to not creating newly unfair outcomes. However I also think that this does not go far enough, and that we also have a responsibility to try to reduce existing unfairness.\n\nDisagreement\n\nFrom the Appendix:\n\nOrganizations and institutions engage in and promote ethical statistical practice by: … Expecting and encouraging all employees and vendors who conduct statistical practice to adhere to these guidelines. Promoting a workplace where the ethical practitioner may apply the guidelines without being intimidated or coerced. Protecting statistical practitioners who comply with these guidelines.\n\nReasoning: First, I am confused and disappointed about why this material is in an Appendix. I think responsibilities at the level of organizations/institutions should be emphasized perhaps even more strongly than at the level of individuals. And second, I think that “expecting and encouraging” and “promoting” statistical practice are too weak, and hard requirements should be considered instead. Perhaps training about ethical statistical practice should be mandatory so that, for example, people working with statisticians understand why they should not try to pressure them to produce different results."
  },
  {
    "objectID": "answers1.html#data-questions",
    "href": "answers1.html#data-questions",
    "title": "Ethics4DS: Coursework 1 answers",
    "section": "Data questions",
    "text": "Data questions\n\nComputing fairness metrics\nUse the fairness package. Pick one of the example datasets in the package. Fit a predictive model using that dataset. Choose three different fairness metrics to compute using the predictions from that model. For each of these, compute the values in the fairness metric in two ways: (1) using standard R functions, e.g. arithmetic operations, and (2) using the fairness package functions. Check to see whether you get the same answer.\n\n# install.packages(\"fairness\")\nlibrary(tidyverse)\nlibrary(fairness)\ndata('germancredit')\n\n\n# Predictive model\ngc_data <- germancredit |>\n  select(BAD, Duration, Amount, Savings,\n         Employment, Installment_rate, Guarantors,\n         Job, Foreign, Female)\ngc_fit <- glm(BAD ~ ., family = binomial(), data = gc_data)\n\n\nFairness metric 1\nWhich metric: demographic parity\n\n# Computing manually in base R\ngc_fit_pred <- gc_data\ngc_fit_pred$.fitted <- predict(gc_fit, type = \"response\")\ninds_F <- which(gc_fit_pred$Female == \"Female\")\ninds_M <- which(gc_fit_pred$Female == \"Male\")\ndata.frame(\n  sex = c(\"Female\", \"Male\"),\n  average = c(mean(gc_fit_pred$.fitted[inds_F]),\n              mean(gc_fit_pred$.fitted[inds_M])),\n  cutoff50 = c(mean(gc_fit_pred$.fitted[inds_F] > 0.5),\n               mean(gc_fit_pred$.fitted[inds_M] > 0.5)),\n  cutoff90 = c(mean(gc_fit_pred$.fitted[inds_F] > 0.9),\n               mean(gc_fit_pred$.fitted[inds_M] > 0.9))\n)\n\n     sex   average  cutoff50   cutoff90\n1 Female 0.7231884 0.9028986 0.10434783\n2   Male 0.6483871 0.8096774 0.04193548\n\n\n\n# Computing manually with tidyverse\ngc_fit_pred <- gc_fit |>\n  broom::augment(type.predict = \"response\")\ngc_fit_pred |>\n  group_by(Female) |>\n  summarize(average = mean(.fitted),\n            cutoff50 = mean(.fitted > 0.5),\n            cutoff90 = mean(.fitted > 0.9))\n\n# A tibble: 2 × 4\n  Female average cutoff50 cutoff90\n  <fct>    <dbl>    <dbl>    <dbl>\n1 Female   0.723    0.903   0.104 \n2 Male     0.648    0.810   0.0419\n\n\n\n# Comparing to the fairness package answer\nfairness_dp <- dem_parity(\n  data = gc_fit_pred,\n  outcome = \"BAD\",\n  group = \"Female\",\n  probs = \".fitted\",\n  cutoff = 0.5\n)\nfairness_dp$Metric\n\n                      Female        Male\nPositively classified    623 251.0000000\nDemographic Parity         1   0.4028892\nGroup size               690 310.0000000\n\n\nUnfortunately this package seems to have a bug currently where the baseline for demographic parity does not take into account the group size. But we can calculate the desired values (proportions of each group with predicted probability above the 0.5 cutoff) this way:\n\nfairness_dp$Metric[1, ] / fairness_dp$Metric[3, ]\n\n   Female      Male \n0.9028986 0.8096774 \n\n\n\n\nFairness metric 2\nWhich metric: false negative rate parity\nTo see which is positive/negative we need to see how the glm function treated the BAD variable (which levels are 0 and 1), so let’s check the predicted probabilities this way:\n\ngc_fit_pred |>\n  group_by(BAD) |>\n  summarize(avg_pred_prob = mean(.fitted))\n\n# A tibble: 2 × 2\n  BAD   avg_pred_prob\n  <fct>         <dbl>\n1 BAD           0.608\n2 GOOD          0.739\n\n\nOn average the model predicts higher probabilities for the GOOD level, so a positive corresponds to good credit.\nIn this case it might make more sense from the perspective of potential customers to have fairness for false negative rates.\nTo compute false negatives we subset to customers with GOOD credit and look at the proportion of them given BAD predictions.\n\n# Computing manually with base R\ninds_GF <- with(gc_fit_pred,\n                which(BAD == \"GOOD\" & Female == \"Female\"))\ninds_GM <- with(gc_fit_pred,\n                which(BAD == \"GOOD\" & Female == \"Male\"))\ndata.frame(\n  sex = c(\"Female\", \"Male\"),\n  cutoff50 = c(mean(gc_fit_pred$.fitted[inds_GF] < 0.5),\n               mean(gc_fit_pred$.fitted[inds_GM] < 0.5)),\n  cutoff90 = c(mean(gc_fit_pred$.fitted[inds_GF] < 0.9),\n               mean(gc_fit_pred$.fitted[inds_GM] < 0.9))\n)\n\n     sex   cutoff50  cutoff90\n1 Female 0.05210421 0.8677355\n2   Male 0.11940299 0.9452736\n\n\n\n# Computing manually with tidyverse\ngc_fit_pred |>\n  dplyr::filter(BAD == \"GOOD\") |> # positives\n  group_by(Female) |>\n  summarize(FN50 = mean(.fitted < .5),\n            FN90 = mean(.fitted < .9))\n\n# A tibble: 2 × 3\n  Female   FN50  FN90\n  <fct>   <dbl> <dbl>\n1 Female 0.0521 0.868\n2 Male   0.119  0.945\n\n\nAt a cutoff of 50% the false negative rates are low but very different by group. At a cutoff of 90% the false negative rates are less different by group but very high.\n\n# Comparing to the fairness package answer\nfairness_fnr <- fnr_parity(\n  data = gc_fit_pred,\n  outcome = \"BAD\",\n  group = \"Female\",\n  probs = \".fitted\",\n  cutoff = 0.5\n)\nfairness_fnr$Metric\n\n                 Female       Male\nFNR          0.05210421   0.119403\nFNR Parity   1.00000000   2.291619\nGroup size 690.00000000 310.000000\n\n\nThe FNR results show the same numbers we computed above as FN50. Verifying for a cutoff of 0.9 would follow the same way.\n\n\n\nSimulating a response variable\nNow replace the outcome variable in the original dataset with a new variable that you generate. You can decide how to generate the new outcome. Your goal is to make this outcome result in all the fairness metrics you chose above indicating that the predictive model is fair.\nAnswer: Recalling the impossibility theorems, we know the only way to satisfy multiple different fairness metrics is (usually, if the fairness metrics are truly different) under some trivial condition like (1) the world is already fair or (2) the model predicts with perfect accuracy.\nThe easiest way to satisfy (1) is to just make the outcome independent of everything.\n\nn <- nrow(gc_data)\ngc_data_sim <- gc_data\ngc_data_sim$BAD <- rbinom(n, 1, .5) # random 0-1\n\nThis is a satisfactory answer since it is technically correct and shows understanding of the application of the impossibility result. But I know that just being technically correct can seem unsatisfying as an answer, so I’ll try to give a more interesting solution as well.\nAnother way to have a “fair world” (satisfying the impossibility theorem) would be if the outcome depends only on predictor variables that are independent of the protected attribute(s). I checked the other numeric predictors in this dataset and none of them were independent of Female, but I saw that I could create a new predictor that was independent, Rate_Duration_ratio:\n\ngc_data_indep <- gc_data |>\n  mutate(Rate_Duration_ratio = Installment_rate / Duration)\ngc_data_indep |>\n  select(-BAD) |>\n  group_by(Female) |>\n  summarize(across(where(is.numeric), mean))\n\n# A tibble: 2 × 5\n  Female Duration Amount Installment_rate Rate_Duration_ratio\n  <fct>     <dbl>  <dbl>            <dbl>               <dbl>\n1 Female     21.6  3448.             3.04               0.190\n2 Male       19.4  2878.             2.83               0.190\n\n\n\nexp_ratio <- function(x) exp(x)/(1+exp(x))\ngc_data_sim <- gc_data_indep |>\n  mutate(\n    BAD = rbinom(n,\n                 1,\n                 prob = exp_ratio(7 * Rate_Duration_ratio)))\ngc_data_sim |>\n  group_by(BAD) |>\n  summarize(avg_ratio = mean(Rate_Duration_ratio))\n\n# A tibble: 2 × 2\n    BAD avg_ratio\n  <int>     <dbl>\n1     0     0.129\n2     1     0.211\n\n\nThis shows we have a predictor variable that is correlated with the outcome, so the outcome is not just purely random and independent of everything (noise).\n\n# Predictive model\ngc_sim_fit <- glm(BAD ~ ., family = binomial(), data = gc_data_sim)\n\n\nFairness metric 1\nWhich metric: demographic parity\n\n# Computing manually\ngc_sim_fit_pred <- gc_sim_fit |>\n  broom::augment(type.predict = \"response\")\ngc_sim_fit_pred |>\n  group_by(Female) |>\n  summarize(average = mean(.fitted),\n            cutoff50 = mean(.fitted > 0.5),\n            cutoff90 = mean(.fitted > 0.9))\n\n# A tibble: 2 × 4\n  Female average cutoff50 cutoff90\n  <fct>    <dbl>    <dbl>    <dbl>\n1 Female   0.746    0.980    0.203\n2 Male     0.742    0.961    0.2  \n\n\n\n# Comparing to the fairness package answer\n# skipped\n\n\n\nFairness metric 2\nWhich metric: false negative rate parity\n\n# Computing manually\n# Computing manually with tidyverse\ngc_sim_fit_pred |>\n  dplyr::filter(BAD == 1) |> # positives\n  group_by(Female) |>\n  summarize(FN50 = mean(.fitted < .5),\n            FN90 = mean(.fitted < .9))\n\n# A tibble: 2 × 3\n  Female   FN50  FN90\n  <fct>   <dbl> <dbl>\n1 Female 0.0175 0.746\n2 Male   0.0174 0.739\n\n\n\n\nConcluding thoughts\nSince the questions were fairly open-ended there are many other possible good answers. This solution guide is just an example of some of the answers that first occurred to me and a few ways of writing of R code that may be helpful to learn from."
  }
]